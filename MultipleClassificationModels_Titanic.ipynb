{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300e148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defining Helper Functions to Train and Evaluate Classification Models --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdea41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# The libraries we will use to implement different ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7549a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch    Fare  Embarked_C  Embarked_Q  \\\n",
       "0         1       2    1   2.0      1      1  26.000           0           0   \n",
       "1         0       3    1  21.0      0      0   7.925           0           0   \n",
       "2         0       3    1  44.0      0      0   8.050           0           0   \n",
       "3         1       3    0  22.0      0      0   7.750           0           0   \n",
       "4         0       3    0  45.0      1      4  27.900           0           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv('datasets/titanic/processed.csv')\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96d8207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked_C',\n",
       " 'Embarked_Q',\n",
       " 'Embarked_S']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every column other than the predicted column (i.e., Survived) is a feature\n",
    "FEATURES = list(titanic_df.columns[1:])\n",
    "\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1cebf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict keys are the different models\n",
    "# dict values are the evaluation metrics \n",
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755b64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that returns the summary metrics for a model\n",
    "def summarize_classification(y_test, y_pred):\n",
    "    # accuracy_score normalize=True gets accuracy as fraction\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    # accuracy_score normalize=False gets number of accurately predicted labels\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    # precision\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    # recall\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': recall,\n",
    "        'accuracy_count': num_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58680a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(classifier_fn, name_of_y_col, names_of_x_cols, dataset, test_frac=0.2):\n",
    "    \"\"\"\n",
    "    A helper function that generalizes the process of building a model.\n",
    "    \n",
    "    :param classifier_fn: A fn that takes x_train and y_train, instantiates an estimator, and trains the model.\n",
    "    :param name_of_y_col: A string name of the df column that contains the target labels to use for training.\n",
    "    :param names_of_x_cols: A string list of the feature names to use for training.\n",
    "    :param dataset: A df that contains the training data.\n",
    "    :param test_frac: The portion of the training data that should be held out for testing\n",
    "    \n",
    "    :return: A dict containing the model summaries for both test and train data, as well as a confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataset[names_of_x_cols]\n",
    "    Y = dataset[name_of_y_col]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_frac)\n",
    "    \n",
    "    model = classifier_fn(x_train, y_train)\n",
    "    \n",
    "    # Predictions on the unseen test data\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Predictions on already seen train data\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Get the summary metrics for the model against both train and test data\n",
    "    train_summary = summarize_classification(y_train, y_pred_train)\n",
    "    test_summary = summarize_classification(y_test, y_pred)\n",
    "    \n",
    "    # Actual vs predicated values\n",
    "    pred_results = pd.DataFrame({\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    \n",
    "    # Confusion matrix\n",
    "    model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "    \n",
    "    return {\n",
    "        'training': train_summary,\n",
    "        'test': test_summary,\n",
    "        'confusion_matrix': model_crosstab\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c903167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that prints the training and test data summaries from each model\n",
    "def compare_results():\n",
    "    for key in result_dict:\n",
    "        print('Classification: ', key)\n",
    "        \n",
    "        print()\n",
    "        print('Training data')\n",
    "        for score in result_dict[key]['training']:\n",
    "            print(score, result_dict[key]['training'][score])\n",
    "            \n",
    "        print()\n",
    "        print('Test data')\n",
    "        for score in result_dict[key]['test']:\n",
    "            print(score, result_dict[key]['test'][score])\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9755e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a logistic regression model\n",
    "def logistic_fn(x_train, y_train):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0665ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ logistic'] = build_model(logistic_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7eeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- ( 4 ) Performing Classification Using Multiple Techniques --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40279b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Implementing Linear Discriminant Analysis Classification (LDA) - #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d538e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = singular value decomposition solver (the default)\n",
    "# svd finds the best axes to fit the data without calculating the covariance matrix of features\n",
    "# useful when we have many features or many rows in a dataset\n",
    "def linear_discriminant_fn(x_train, y_train, solver='svd'):\n",
    "    \n",
    "    model = LinearDiscriminantAnalysis(solver=solver)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f16dfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7855887521968365\n",
      "precision 0.7476635514018691\n",
      "recall 0.7017543859649122\n",
      "accuracy_count 447\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.7894736842105263\n",
      "recall 0.75\n",
      "accuracy_count 116\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# The tutorial received a warning about collinearity that did not occur in my copy\\n# This problem is common when a dataset has one-hot encoded features and includes all of them in the training data\\n# It can cause a dummy trap, a perfect collinearity between 2 or more features\\n# This can often be solved through \"dummy encoding\" where we drop one of the one-hot encoded columns (see below)\\n# Some estimators automatically handle this, it\\'s possible this version of scikit now handles it, thus no warning\\n# However, the tutorial was posted June 2019 and this scikit commit from July 2019 seems to just remove the warning:\\n# https://github.com/scikit-learn/scikit-learn/issues/14361\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()\n",
    "\n",
    "\"\"\"\n",
    "# The tutorial received a warning about collinearity that did not occur in my copy\n",
    "# This problem is common when a dataset has one-hot encoded features and includes all of them in the training data\n",
    "# It can cause a dummy trap, a perfect collinearity between 2 or more features\n",
    "# This can often be solved through \"dummy encoding\" where we drop one of the one-hot encoded columns (see below)\n",
    "# Some estimators automatically handle this, it's possible this version of scikit now handles it, thus no warning\n",
    "# However, the tutorial was posted June 2019 and this scikit commit from July 2019 seems to just remove the warning:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/14361\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd62f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FEATURES[0:-1] drops the last column\n",
    "result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0841cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Implementing Quadratic Discriminant Analysis Classification (QDA) - #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "986a59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find axes to best separate the classes such that all instances of a class are in the same quadrant \n",
    "# but the decision boundary is quadratic\n",
    "\n",
    "# Useful when the X variables corresponding to different labels have different covariances\n",
    "# i.e., covariances are different for X for all values of Y\n",
    "def quadratic_discriminant_fn(x_train, y_train):\n",
    "    \n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(x_train, y_train)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a84c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note prone to dummy trap and last column is dropped\n",
    "result_dict['survived ~ quadratic_discriminant_analysis'] = build_model(quadratic_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "798c7c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Logistic Regression model:\\nLoss (Cost) Function is the cross entropy, it measures how well the estimated probabilities match the actual labels.\\nThe training process will try to minimize cross entropy.\\n\\nCross entropy diagram with two varibes but three lines:\\n- w (weights) - x axis\\n- b (biases) - diagnal from corner\\n- cross entropy - y axis\\n\\nStochastic: Randomly determined\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Implementing Stochastic Gradient Descent Classifiers (SGD) --- #\n",
    "\n",
    "\"\"\"\n",
    "In Logistic Regression model:\n",
    "Loss (Cost) Function is the cross entropy, it measures how well the estimated probabilities match the actual labels.\n",
    "The training process will try to minimize cross entropy.\n",
    "\n",
    "Cross entropy diagram with two varibes but three lines:\n",
    "- w (weights) - x axis\n",
    "- b (biases) - diagnal from corner\n",
    "- cross entropy - y axis\n",
    "\n",
    "Stochastic: Randomly determined\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfaa8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively converges to the best model\n",
    "# Performs numerical optimization - one training instance at a time to find the best model parameters\n",
    "# Specify hyperparameters to help design the right model for use case\n",
    "    # max_iter max number of iteration for which the model should train\n",
    "    # tol: the stopping criteria for training (if the change of loss falls below tol, the model is no longer improving)\n",
    "def sgd_fn(x_train, y_train, max_iter=10000, tol=1e-3):\n",
    "    \n",
    "    model = SGDClassifier(max_iter=max_iter, tol=tol)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d9696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ sgd'] = build_model(sgd_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "# Tutorial started with sgd_fn max_iter 1000 with low accuracy then 10000 produced higher accuracy\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
