{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c6a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Defining Helper Functions to Train and Evaluate Classification Models --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe4e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# The libraries we will use to implement different ML models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04eb863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch    Fare  Embarked_C  Embarked_Q  \\\n",
       "0         1       2    1   2.0      1      1  26.000           0           0   \n",
       "1         0       3    1  21.0      0      0   7.925           0           0   \n",
       "2         0       3    1  44.0      0      0   8.050           0           0   \n",
       "3         1       3    0  22.0      0      0   7.750           0           0   \n",
       "4         0       3    0  45.0      1      4  27.900           0           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv('datasets/titanic/processed.csv')\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deaefb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked_C',\n",
       " 'Embarked_Q',\n",
       " 'Embarked_S']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every column other than the predicted column (i.e., Survived) is a feature\n",
    "FEATURES = list(titanic_df.columns[1:])\n",
    "\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict keys are the different models\n",
    "# dict values are the evaluation metrics \n",
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b72ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that returns the summary metrics for a model\n",
    "def summarize_classification(y_test, y_pred):\n",
    "    # accuracy_score normalize=True gets accuracy as fraction\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    # accuracy_score normalize=False gets number of accurately predicted labels\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    # precision\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    # recall\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': recall,\n",
    "        'accuracy_count': num_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f6e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(classifier_fn, name_of_y_col, names_of_x_cols, dataset, test_frac=0.2):\n",
    "    \"\"\"\n",
    "    A helper function that generalizes the process of building a model.\n",
    "    \n",
    "    :param classifier_fn: A fn that takes x_train and y_train, instantiates an estimator, and trains the model.\n",
    "    :param name_of_y_col: A string name of the df column that contains the target labels to use for training.\n",
    "    :param names_of_x_cols: A string list of the feature names to use for training.\n",
    "    :param dataset: A df that contains the training data.\n",
    "    :param test_frac: The portion of the training data that should be held out for testing\n",
    "    \n",
    "    :return: A dict containing the model summaries for both test and train data, as well as a confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataset[names_of_x_cols]\n",
    "    Y = dataset[name_of_y_col]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_frac)\n",
    "    \n",
    "    model = classifier_fn(x_train, y_train)\n",
    "    \n",
    "    # Predictions on the unseen test data\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    # Predictions on already seen train data\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    # Get the summary metrics for the model against both train and test data\n",
    "    train_summary = summarize_classification(y_train, y_pred_train)\n",
    "    test_summary = summarize_classification(y_test, y_pred)\n",
    "    \n",
    "    # Actual vs predicated values\n",
    "    pred_results = pd.DataFrame({\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    \n",
    "    # Confusion matrix\n",
    "    model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "    \n",
    "    return {\n",
    "        'training': train_summary,\n",
    "        'test': test_summary,\n",
    "        'confusion_matrix': model_crosstab\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b46a058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that prints the training and test data summaries from each model\n",
    "def compare_results():\n",
    "    for key in result_dict:\n",
    "        print('Classification: ', key)\n",
    "        \n",
    "        print()\n",
    "        print('Training data')\n",
    "        for score in result_dict[key]['training']:\n",
    "            print(score, result_dict[key]['training'][score])\n",
    "            \n",
    "        print()\n",
    "        print('Test data')\n",
    "        for score in result_dict[key]['test']:\n",
    "            print(score, result_dict[key]['test'][score])\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c0935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train a logistic regression model\n",
    "def logistic_fn(x_train, y_train):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728c2964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ logistic'] = build_model(logistic_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d88a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- ( 4 ) Performing Classification Using Multiple Techniques --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1e5fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Implementing Linear Discriminant Analysis Classification (LDA) - #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c3d5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = singular value decomposition solver (the default)\n",
    "# svd finds the best axes to fit the data without calculating the covariance matrix of features\n",
    "# useful when we have many features or many rows in a dataset\n",
    "def linear_discriminant_fn(x_train, y_train, solver='svd'):\n",
    "    \n",
    "    model = LinearDiscriminantAnalysis(solver=solver)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9fa091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7855887521968365\n",
      "precision 0.7476635514018691\n",
      "recall 0.7017543859649122\n",
      "accuracy_count 447\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.7894736842105263\n",
      "recall 0.75\n",
      "accuracy_count 116\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# The tutorial received a warning about collinearity that did not occur in my copy\\n# This problem is common when a dataset has one-hot encoded features and includes all of them in the training data\\n# It can cause a dummy trap, a perfect collinearity between 2 or more features\\n# This can often be solved through \"dummy encoding\" where we drop one of the one-hot encoded columns (see below)\\n# Some estimators automatically handle this, it\\'s possible this version of scikit now handles it, thus no warning\\n# However, the tutorial was posted June 2019 and this scikit commit from July 2019 seems to just remove the warning:\\n# https://github.com/scikit-learn/scikit-learn/issues/14361\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()\n",
    "\n",
    "\"\"\"\n",
    "# The tutorial received a warning about collinearity that did not occur in my copy\n",
    "# This problem is common when a dataset has one-hot encoded features and includes all of them in the training data\n",
    "# It can cause a dummy trap, a perfect collinearity between 2 or more features\n",
    "# This can often be solved through \"dummy encoding\" where we drop one of the one-hot encoded columns (see below)\n",
    "# Some estimators automatically handle this, it's possible this version of scikit now handles it, thus no warning\n",
    "# However, the tutorial was posted June 2019 and this scikit commit from July 2019 seems to just remove the warning:\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/14361\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b969e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FEATURES[0:-1] drops the last column\n",
    "result_dict['survived ~ linear_discriminant_analysis'] = build_model(linear_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5e759e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Implementing Quadratic Discriminant Analysis Classification (QDA) - #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5ffb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find axes to best separate the classes such that all instances of a class are in the same quadrant \n",
    "# but the decision boundary is quadratic\n",
    "\n",
    "# Useful when the X variables corresponding to different labels have different covariances\n",
    "# i.e., covariances are different for X for all values of Y\n",
    "def quadratic_discriminant_fn(x_train, y_train):\n",
    "    \n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(x_train, y_train)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff869e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note prone to dummy trap and last column is dropped\n",
    "result_dict['survived ~ quadratic_discriminant_analysis'] = build_model(quadratic_discriminant_fn, 'Survived', FEATURES[0:-1], titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f468a2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Logistic Regression model:\\nLoss (Cost) Function is the cross entropy, it measures how well the estimated probabilities match the actual labels.\\nThe training process will try to minimize cross entropy.\\n\\nCross entropy diagram with two varibes but three lines:\\n- w (weights) - x axis\\n- b (biases) - diagnal from corner\\n- cross entropy - y axis\\n\\nStochastic: Randomly determined\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Implementing Stochastic Gradient Descent Classifiers (SGD) --- #\n",
    "\n",
    "\"\"\"\n",
    "In Logistic Regression model:\n",
    "Loss (Cost) Function is the cross entropy, it measures how well the estimated probabilities match the actual labels.\n",
    "The training process will try to minimize cross entropy.\n",
    "\n",
    "Cross entropy diagram with two varibes but three lines:\n",
    "- w (weights) - x axis\n",
    "- b (biases) - diagnal from corner\n",
    "- cross entropy - y axis\n",
    "\n",
    "Stochastic: Randomly determined\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5495858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively converges to the best model\n",
    "# Performs numerical optimization - one training instance at a time to find the best model parameters\n",
    "# Specify hyperparameters to help design the right model for use case\n",
    "    # max_iter max number of iteration for which the model should train\n",
    "    # tol: the stopping criteria for training (if the change of loss falls below tol, the model is no longer improving)\n",
    "def sgd_fn(x_train, y_train, max_iter=10000, tol=1e-3):\n",
    "    \n",
    "    model = SGDClassifier(max_iter=max_iter, tol=tol)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3371ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ sgd'] = build_model(sgd_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "# Tutorial started with sgd_fn max_iter 1000 with low accuracy then 10000 produced higher accuracy\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f6c841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Support Vector Machines --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96358615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a hyperplane that separates points so all points on the same side belong to the same class\n",
    "def linear_svc_fn(x_train, y_train, C=1.0, max_iter=1000, tol=1e-3):\n",
    "    \"\"\"\n",
    "    C specifies the inverse strength of the regularization (smaller values indicate stronger reguarlization)\n",
    "    Penalize points on the wrong side of the margin\n",
    "    \n",
    "    tol describes number that will stop training after 2 consecutive iterations of sub tol improvement\n",
    "    \n",
    "    dual is a optimization setting that could convert primal to dual, dual is easier to solve using optimization\n",
    "    prefer dual=False when n_samples > n_features\n",
    "    \"\"\"\n",
    "    \n",
    "    # LinearSVC == SVC(kernel=\"linear\")\n",
    "    # Kernal referes to data tranformation performed by estimator so model is easier to optimize\n",
    "    model = LinearSVC(C=C, max_iter=max_iter, tol=tol, dual=False)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cce77db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ linear_svc\n",
      "\n",
      "Training data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7782805429864253\n",
      "recall 0.7288135593220338\n",
      "accuracy_count 456\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.6666666666666666\n",
      "recall 0.6538461538461539\n",
      "accuracy_count 108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ linear_svc'] = build_model(linear_svc_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e795169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUses training data to find what is most similiar to the current sample\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Implementing K-nearest-neighbors classification --- #\n",
    "\n",
    "\"\"\"\n",
    "Uses training data to find what is most similiar to the current sample\n",
    "Predictions for a new sample involves figuring our which element in the training data it is similiar to \n",
    "Distance (Euclidean)\n",
    "\n",
    "1. K-nearest-neighbors (voting among K nearest neighbors)\n",
    "2. Radius Neighbors (voting among all neighbors within radius)\n",
    "\n",
    "Use hyperparameter tuning on K or radius to improve results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2121ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for neighbors within the specified radius\n",
    "def radius_neighbor_fn(x_train, y_train, radius=40.0): \n",
    "    \n",
    "    model = RadiusNeighborsClassifier(radius=radius)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c09259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ linear_svc\n",
      "\n",
      "Training data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7782805429864253\n",
      "recall 0.7288135593220338\n",
      "accuracy_count 456\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.6666666666666666\n",
      "recall 0.6538461538461539\n",
      "accuracy_count 108\n",
      "\n",
      "Classification:  survived ~ radius_neighbors\n",
      "\n",
      "Training data\n",
      "accuracy 0.6660808435852372\n",
      "precision 0.7127659574468085\n",
      "recall 0.29130434782608694\n",
      "accuracy_count 379\n",
      "\n",
      "Test data\n",
      "accuracy 0.6993006993006993\n",
      "precision 0.8260869565217391\n",
      "recall 0.3275862068965517\n",
      "accuracy_count 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ radius_neighbors'] = build_model(radius_neighbor_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbfc3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implementing Decision Tree classifiers --- #\n",
    "\"\"\"\n",
    "Set up a tree structure on training data ewhich helps make decision based on rules\n",
    "\n",
    "Can be prone to overfitting, seeing high accuracy for train data but not test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0653503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree to training data using CART (Classification and Regression Tree) algorithm\n",
    "def decision_tree_fn(x_train, y_train, max_depth=None, max_features=None):\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, max_features=max_features)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8334f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ linear_svc\n",
      "\n",
      "Training data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7782805429864253\n",
      "recall 0.7288135593220338\n",
      "accuracy_count 456\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.6666666666666666\n",
      "recall 0.6538461538461539\n",
      "accuracy_count 108\n",
      "\n",
      "Classification:  survived ~ radius_neighbors\n",
      "\n",
      "Training data\n",
      "accuracy 0.6660808435852372\n",
      "precision 0.7127659574468085\n",
      "recall 0.29130434782608694\n",
      "accuracy_count 379\n",
      "\n",
      "Test data\n",
      "accuracy 0.6993006993006993\n",
      "precision 0.8260869565217391\n",
      "recall 0.3275862068965517\n",
      "accuracy_count 100\n",
      "\n",
      "Classification:  survived ~ decision_tree\n",
      "\n",
      "Training data\n",
      "accuracy 0.9876977152899824\n",
      "precision 1.0\n",
      "recall 0.9694323144104804\n",
      "accuracy_count 562\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.7142857142857143\n",
      "recall 0.6779661016949152\n",
      "accuracy_count 108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ decision_tree'] = build_model(decision_tree_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a77e51f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nA Priori Probabilities are general probabilities before knowing anything specific about a given sample\\nConditional probabilities are specific to the sample\\n\\nMakes naive (strong) assumptions about independence of features (doesn't take into account that features could be related)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Implementing Naive Bayes Classifieres--- #\n",
    "\"\"\"\n",
    "A Priori Probabilities are general probabilities before knowing anything specific about a given sample\n",
    "Conditional probabilities are specific to the sample\n",
    "\n",
    "Makes naive (strong) assumptions about independence of features (doesn't take into account that features could be related)\n",
    "\n",
    "Can create robust models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96c3f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bayes' theorem to find which label is most likely given the attributes observed in the feature vector\n",
    "# and given how often the different labels occur in the data\n",
    "def naive_bayes_fn(x_train, y_train, priors=None):\n",
    "    \n",
    "    model = GaussianNB(priors=priors)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5514806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification:  survived ~ logistic\n",
      "\n",
      "Training data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.7881773399014779\n",
      "recall 0.6926406926406926\n",
      "accuracy_count 455\n",
      "\n",
      "Test data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.8235294117647058\n",
      "recall 0.7368421052631579\n",
      "accuracy_count 119\n",
      "\n",
      "Classification:  survived ~ linear_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.8031634446397188\n",
      "precision 0.7772511848341233\n",
      "recall 0.7161572052401747\n",
      "accuracy_count 457\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.7090909090909091\n",
      "recall 0.6610169491525424\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ quadratic_discriminant_analysis\n",
      "\n",
      "Training data\n",
      "accuracy 0.7943760984182777\n",
      "precision 0.7757847533632287\n",
      "recall 0.7208333333333333\n",
      "accuracy_count 452\n",
      "\n",
      "Test data\n",
      "accuracy 0.8111888111888111\n",
      "precision 0.723404255319149\n",
      "recall 0.7083333333333334\n",
      "accuracy_count 116\n",
      "\n",
      "Classification:  survived ~ sgd\n",
      "\n",
      "Training data\n",
      "accuracy 0.7627416520210897\n",
      "precision 0.6619217081850534\n",
      "recall 0.8230088495575221\n",
      "accuracy_count 434\n",
      "\n",
      "Test data\n",
      "accuracy 0.7482517482517482\n",
      "precision 0.696969696969697\n",
      "recall 0.7419354838709677\n",
      "accuracy_count 107\n",
      "\n",
      "Classification:  survived ~ linear_svc\n",
      "\n",
      "Training data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7782805429864253\n",
      "recall 0.7288135593220338\n",
      "accuracy_count 456\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.6666666666666666\n",
      "recall 0.6538461538461539\n",
      "accuracy_count 108\n",
      "\n",
      "Classification:  survived ~ radius_neighbors\n",
      "\n",
      "Training data\n",
      "accuracy 0.6660808435852372\n",
      "precision 0.7127659574468085\n",
      "recall 0.29130434782608694\n",
      "accuracy_count 379\n",
      "\n",
      "Test data\n",
      "accuracy 0.6993006993006993\n",
      "precision 0.8260869565217391\n",
      "recall 0.3275862068965517\n",
      "accuracy_count 100\n",
      "\n",
      "Classification:  survived ~ decision_tree\n",
      "\n",
      "Training data\n",
      "accuracy 0.9876977152899824\n",
      "precision 1.0\n",
      "recall 0.9694323144104804\n",
      "accuracy_count 562\n",
      "\n",
      "Test data\n",
      "accuracy 0.7552447552447552\n",
      "precision 0.7142857142857143\n",
      "recall 0.6779661016949152\n",
      "accuracy_count 108\n",
      "\n",
      "Classification:  survived ~ naive_bayes\n",
      "\n",
      "Training data\n",
      "accuracy 0.7820738137082601\n",
      "precision 0.755\n",
      "recall 0.668141592920354\n",
      "accuracy_count 445\n",
      "\n",
      "Test data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.7413793103448276\n",
      "recall 0.6935483870967742\n",
      "accuracy_count 109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived ~ naive_bayes'] = build_model(naive_bayes_fn, 'Survived', FEATURES, titanic_df)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Hyperparameter Tuning for Classification Models --- #\n",
    "# See notebooks/HyperparameterTuningWithGridSearch.ipynb\n",
    "\"\"\"\n",
    "Hyperparameters are model configuration properties that define a model and remain constant during training\n",
    "They are part of the model design and do not change\n",
    "\n",
    "Model Inputs - train data (this trains the parameters)\n",
    "Model Parameters - found during training (these are learned, e.g., model coefficient and intercept)\n",
    "Model Hyperparameters - part of the model design (e.g., depth of tree, k neighbors)\n",
    "\n",
    "Grid search is a scikit utility that creates a grid of possible values for each hyperparameter, each cell is a candidate model\n",
    "gridsearchcv evaluates each candidate model (using cross validation)\n",
    "It is computationally very expensive (also actual cost in cloud can be expensive)\n",
    "Does not differentiate between important and trivial hyperparameters\n",
    "\n",
    "Alternatively, random search of hyperparameter space can be done\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
